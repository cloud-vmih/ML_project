import numpy as np

class RidgeRegressionMetaModel:
    """
    Ridge Regression Meta-Model implement t·ª´ ƒë·∫ßu
    D√πng ƒë·ªÉ combine predictions t·ª´ base models
    """
    
    def __init__(self, alpha=0.5, fit_intercept=True):
        """
        Kh·ªüi t·∫°o Ridge Regression Meta-Model
        
        Parameters:
        -----------
        alpha : float
            Regularization strength (Œª)
            alpha c√†ng l·ªõn, regularization c√†ng m·∫°nh
        fit_intercept : bool
            C√≥ th√™m bias term (intercept) hay kh√¥ng
        """
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.weights = None  # Vector tr·ªçng s·ªë w
        self.bias = 0.0  # Bias term (n·∫øu c√≥)
        
    def _add_intercept(self, X):
        """Th√™m c·ªôt 1 cho intercept (bias term)"""
        if self.fit_intercept:
            return np.c_[np.ones(X.shape[0]), X]
        return X
    
    def _remove_intercept(self):
        """T√°ch bias t·ª´ weights n·∫øu c√≥ intercept"""
        if self.fit_intercept and self.weights is not None:
            self.bias = self.weights[0]
            self.weights = self.weights[1:]
    
    def _closed_form_solution(self, X, y):
        """
        Gi·∫£i Ridge Regression b·∫±ng c√¥ng th·ª©c ƒë√≥ng
        
        C√¥ng th·ª©c: w = (X^T X + Œ±I)^(-1) X^T y
        
        Returns:
            weights: Vector tr·ªçng s·ªë
        """
        n_samples, n_features = X.shape
        
        # X^T X
        XTX = X.T @ X
        
        # T·∫°o ma tr·∫≠n identity I
        I = np.eye(n_features)
        
        # N·∫øu c√≥ intercept, kh√¥ng regularize bias term
        if self.fit_intercept:
            I[0, 0] = 0  # Kh√¥ng regularize c·ªôt intercept
        
        # (X^T X + Œ±I)
        XTX_regularized = XTX + self.alpha * I
        
        # (X^T X + Œ±I)^(-1) X^T y
        try:
            # Inverse c·ªßa ma tr·∫≠n
            weights = np.linalg.inv(XTX_regularized) @ X.T @ y
        except np.linalg.LinAlgError:
            # N·∫øu ma tr·∫≠n singular, d√πng pseudoinverse
            print("Warning: Matrix is singular, using pseudoinverse")
            weights = np.linalg.pinv(XTX_regularized) @ X.T @ y
        
        return weights
    
    def _gradient_descent(self, X, y, learning_rate=0.01, n_iterations=1000):
        """
        Gi·∫£i Ridge Regression b·∫±ng Gradient Descent
        
        Loss function: L(w) = ||y - Xw||¬≤ + Œ±||w||¬≤
        Gradient: ‚àáL(w) = -2X^T(y - Xw) + 2Œ±w
        """
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        
        # L∆∞u loss ƒë·ªÉ theo d√µi
        losses = []
        
        for i in range(n_iterations):
            # D·ª± ƒëo√°n
            y_pred = X @ self.weights
            
            # T√≠nh loss (MSE + regularization)
            mse = np.mean((y - y_pred) ** 2)
            reg_term = self.alpha * np.sum(self.weights ** 2)
            loss = mse + reg_term
            losses.append(loss)
            
            # T√≠nh gradient
            # Gradient c·ªßa MSE: -2 * X^T(y - Xw) / n_samples
            mse_grad = -2 * X.T @ (y - y_pred) / n_samples
            
            # Gradient c·ªßa regularization: 2Œ±w
            reg_grad = 2 * self.alpha * self.weights
            
            # Gradient t·ªïng
            gradient = mse_grad + reg_grad
            
            # Update weights
            self.weights -= learning_rate * gradient
            
            # In loss m·ªói 100 iterations
            if i % 100 == 0:
                print(f"Iteration {i}: Loss = {loss:.6f}")
        
        return losses
    
    def fit(self, X_meta, y, method='closed_form', **kwargs):
        """
        Hu·∫•n luy·ªán Ridge Regression Meta-Model
        
        Parameters:
        -----------
        X_meta : numpy array, shape (n_samples, n_base_models)
            Meta-features t·ª´ base models predictions
        y : numpy array, shape (n_samples,)
            Target values (ratings)
        method : str
            Ph∆∞∆°ng ph√°p gi·∫£i: 'closed_form' ho·∫∑c 'gradient_descent'
        **kwargs : dict
            Tham s·ªë cho gradient descent (learning_rate, n_iterations)
        """
        # Chuy·ªÉn ƒë·ªïi th√†nh numpy array
        X_meta = np.array(X_meta, dtype=np.float64)
        y = np.array(y, dtype=np.float64).flatten()
        
        # Th√™m intercept n·∫øu c·∫ßn
        X_with_intercept = self._add_intercept(X_meta)
        
        # L∆∞u s·ªë base models
        self.n_base_models = X_meta.shape[1]
        
        print(f"Training Ridge Meta-Model (Œ±={self.alpha})...")
        print(f"  Samples: {X_meta.shape[0]}")
        print(f"  Base models: {self.n_base_models}")
        print(f"  Method: {method}")
        
        if method == 'closed_form':
            # Gi·∫£i b·∫±ng c√¥ng th·ª©c ƒë√≥ng
            weights = self._closed_form_solution(X_with_intercept, y)
            self.weights = weights
            
            # T√°ch bias n·∫øu c√≥ intercept
            if self.fit_intercept:
                self.bias = weights[0]
                self.weights = weights[1:]
                
        elif method == 'gradient_descent':
            # Gi·∫£i b·∫±ng gradient descent
            learning_rate = kwargs.get('learning_rate', 0.01)
            n_iterations = kwargs.get('n_iterations', 1000)
            
            losses = self._gradient_descent(
                X_with_intercept, y, 
                learning_rate, n_iterations
            )
            
            # T√°ch bias n·∫øu c√≥ intercept
            if self.fit_intercept:
                self.bias = self.weights[0]
                self.weights = self.weights[1:]
            
            self.loss_history = losses
            
        else:
            raise ValueError(f"Unknown method: {method}. Use 'closed_form' or 'gradient_descent'")
        
        # T√≠nh R¬≤ score
        y_pred = self.predict(X_meta)
        self.r2_score = self._calculate_r2(y, y_pred)
        
        print(f"  Training completed!")
        print(f"  R¬≤ score: {self.r2_score:.4f}")
        print(f"  Weights: {self.weights}")
        if self.fit_intercept:
            print(f"  Bias: {self.bias:.4f}")
        
        return self
    
    def predict(self, X_meta):
        """
        D·ª± ƒëo√°n v·ªõi meta-model
        
        Parameters:
        -----------
        X_meta : numpy array, shape (n_samples, n_base_models)
            Predictions t·ª´ base models
        
        Returns:
        --------
        y_pred : numpy array
            D·ª± ƒëo√°n cu·ªëi c√πng
        """
        X_meta = np.array(X_meta, dtype=np.float64)
        
        # D·ª± ƒëo√°n: y = Xw + b
        y_pred = X_meta @ self.weights
        
        if self.fit_intercept:
            y_pred += self.bias
            
        return y_pred
    
    def _calculate_r2(self, y_true, y_pred):
        """T√≠nh R¬≤ score"""
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        r2 = 1 - (ss_res / (ss_tot + 1e-10))  # Th√™m epsilon ƒë·ªÉ tr√°nh chia 0
        return r2
    
    def get_feature_importance(self):
        """
        L·∫•y ƒë·ªô quan tr·ªçng c·ªßa t·ª´ng base model
        
        Returns:
        --------
        importance : dict
            Dictionary v·ªõi base model v√† weight t∆∞∆°ng ·ª©ng
        """
        if self.weights is None:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c trained")
        
        importance = {}
        for i, weight in enumerate(self.weights):
            model_name = f"Base_Model_{i+1}"
            importance[model_name] = float(weight)
        
        # Normalize ƒë·ªÉ t·ªïng absolute weights = 1
        total_abs = np.sum(np.abs(list(importance.values())))
        if total_abs > 0:
            for model in importance:
                importance[model] = importance[model] / total_abs
        
        return importance
    
    def get_final_formula(self, base_model_names=None):
        """
        L·∫•y c√¥ng th·ª©c cu·ªëi c√πng c·ªßa meta-model
        
        Parameters:
        -----------
        base_model_names : list
            T√™n c·ªßa c√°c base models
        
        Returns:
        --------
        formula : str
            C√¥ng th·ª©c d·ª± ƒëo√°n
        """
        if base_model_names is None:
            base_model_names = [f"f{i+1}(x)" for i in range(self.n_base_models)]
        
        formula_parts = []
        
        # Th√™m bias term n·∫øu c√≥
        if self.fit_intercept and abs(self.bias) > 1e-10:
            formula_parts.append(f"{self.bias:.4f}")
        
        # Th√™m c√°c base model terms
        for i, (name, weight) in enumerate(zip(base_model_names, self.weights)):
            if abs(weight) > 1e-10:  # Ch·ªâ th√™m n·∫øu weight ƒë√°ng k·ªÉ
                sign = "+" if weight >= 0 else "-"
                abs_weight = abs(weight)
                formula_parts.append(f"{sign} {abs_weight:.4f}√ó{name}")
        
        formula = "≈∑ = " + " ".join(formula_parts)
        return formula


# ============================================================================
# STACKING PIPELINE HO√ÄN CH·ªàNH V·ªöI RIDGE META-MODEL
# ============================================================================





# # ============================================================================
# # DEMO V√Ä TEST
# # ============================================================================

# def test_ridge_regression():
#     """Test Ridge Regression Meta-Model"""
#     print("\n" + "="*60)
#     print("TEST RIDGE REGRESSION META-MODEL")
#     print("="*60)
    
#     # T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p: 3 base models, 100 samples
#     np.random.seed(42)
#     n_samples = 100
#     n_models = 3
    
#     # T·∫°o predictions t·ª´ base models
#     X_meta = np.random.randn(n_samples, n_models) * 0.5
    
#     # T·∫°o target: k·∫øt h·ª£p tuy·∫øn t√≠nh c·ªßa predictions + noise
#     true_weights = np.array([0.3, 0.5, 0.2])
#     bias = 0.5
#     y = X_meta @ true_weights + bias + np.random.randn(n_samples) * 0.1
    
#     # T·∫°o v√† train Ridge Regression meta-model
#     ridge_model = RidgeRegressionMetaModel(alpha=0.1, fit_intercept=True)
#     ridge_model.fit(X_meta, y, method='closed_form')
    
#     # D·ª± ƒëo√°n
#     y_pred = ridge_model.predict(X_meta)
    
#     # ƒê√°nh gi√°
#     mse = np.mean((y - y_pred) ** 2)
#     r2 = ridge_model.r2_score
    
#     print(f"\nResults:")
#     print(f"  True weights: {true_weights}")
#     print(f"  True bias: {bias}")
#     print(f"  Learned weights: {ridge_model.weights}")
#     print(f"  Learned bias: {ridge_model.bias}")
#     print(f"  MSE: {mse:.6f}")
#     print(f"  R¬≤: {r2:.4f}")
    
#     # Feature importance
#     importance = ridge_model.get_feature_importance()
#     print(f"\nFeature Importance:")
#     for model, weight in importance.items():
#         print(f"  {model}: {weight:.4f}")
    
#     # Final formula
#     formula = ridge_model.get_final_formula(['f‚ÇÅ(x)', 'f‚ÇÇ(x)', 'f‚ÇÉ(x)'])
#     print(f"\nFinal Formula:")
#     print(f"  {formula}")
    
#     return ridge_model

# def test_stacking_ensemble():
#     """Test to√†n b·ªô Stacking Ensemble"""
#     print("\n" + "="*60)
#     print("TEST STACKING ENSEMBLE")
#     print("="*60)
    
#     # T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p
#     np.random.seed(42)
#     n_samples = 200
#     n_features = 5
    
#     # Features
#     X = np.random.randn(n_samples, n_features)
    
#     # Target: h√†m phi tuy·∫øn
#     y = (X[:, 0] ** 2 + np.sin(X[:, 1]) + 
#          X[:, 2] * X[:, 3] + np.random.randn(n_samples) * 0.5)
    
#     # T·∫°o base models
#     base_models = [
#         SimpleLinearModel(),
#         SimpleKNN(k=7),
#         SimpleDecisionTree(max_depth=4)
#     ]
    
#     # T·∫°o Stacking Ensemble
#     stacking = ManualStackingEnsemble(
#         base_models=base_models,
#         meta_model=RidgeRegressionMetaModel(alpha=0.5, fit_intercept=True),
#         n_folds=3
#     )
    
#     # Train
#     stacking.fit(X, y)
    
#     # Predict
#     y_pred = stacking.predict(X)
    
#     # ƒê√°nh gi√°
#     mse = np.mean((y - y_pred) ** 2)
#     print(f"\nStacking Ensemble Performance:")
#     print(f"  MSE: {mse:.6f}")
    
#     # So s√°nh v·ªõi base models
#     print(f"\nBase Models Performance:")
#     for i, model in enumerate(base_models):
#         model_pred = model.predict(X)
#         model_mse = np.mean((y - model_pred) ** 2)
#         print(f"  Model {i+1}: MSE = {model_mse:.6f}")
    
#     # Summary
#     stacking.get_model_summary()
    
#     return stacking

# if __name__ == "__main__":
#     print("üéØ IMPLEMENTING RIDGE REGRESSION META-MODEL FROM SCRATCH")
    
#     # Test 1: Ridge Regression Meta-Model
#     ridge_model = test_ridge_regression()
    
#     # Test 2: Full Stacking Ensemble
#     stacking_model = test_stacking_ensemble()
    
#     print("\n" + "="*60)
#     print("ALL TESTS COMPLETED SUCCESSFULLY! ‚úÖ")
#     print("="*60)